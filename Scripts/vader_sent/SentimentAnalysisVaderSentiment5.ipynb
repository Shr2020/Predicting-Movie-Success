{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import nltk \n",
    "import string\n",
    "import re\n",
    "#from langdetect import detect\n",
    "import os\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punct(text):\n",
    "#     text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "#     text = re.sub('[0-9]+', '', text)\n",
    "#     return text\n",
    "\n",
    "# def tokenization(text):\n",
    "#     text = re.split('\\W+', text)\n",
    "#     return text\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     text = [word for word in text if word not in stopword]\n",
    "#     return text\n",
    "\n",
    "\n",
    "# ps = nltk.PorterStemmer()\n",
    "\n",
    "# def stemming(text):\n",
    "#     text = [ps.stem(word) for word in text]\n",
    "#     return text\n",
    "\n",
    "# wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# def lemmatizer(text):\n",
    "#     text = [wn.lemmatize(word) for word in text]\n",
    "#     return text\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "#     text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "#     tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "#     text = [ps.stem(word) for word in tokens if word not in stopword]# remove stopwords and stemming\n",
    "#     sentence = ' '.join(text) # join list in a sentence\n",
    "#     return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieSentiment:\n",
    "    def __init__(self, name):\n",
    "        self.movie_name = name\n",
    "        self.pos = 0\n",
    "        self.n = 0\n",
    "        self.neg = 0\n",
    "        self.tweets = 0\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"Movie:\", self.movie_name, \"| Tweets analysed:\", self.tweets,  \"| Postive:\", self.pos, \"| Negative:\", self.neg, \"| Neutral:\", self.n)\n",
    "    \n",
    "movies = []\n",
    "movie_class = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleanup(tweet):\n",
    "#     s = re.sub(r'@\\S+', '', tweet)\n",
    "#     s = re.sub(r'http\\S+', '', s)\n",
    "#     s = re.sub('\\.{3}', '.', s)\n",
    "#     s = re.sub(r'[^\\x00-\\x7f]',r'', s)\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    " \n",
    "# function to print sentiments\n",
    "# of the sentence.\n",
    "def sentiment_scores(sentence):\n",
    "    try:\n",
    "        sid_obj = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "        overall_sent = 0\n",
    "    #     print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    #     print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    #     print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    #     print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "    #     print(\"Sentence Overall Rated As\", end = \" \")\n",
    "\n",
    "        if sentiment_dict['compound'] >= 0.05 :\n",
    "            #print(\"Positive\")\n",
    "            overall_sent = 1\n",
    "        elif sentiment_dict['compound'] <= - 0.05 :\n",
    "            #print(\"Negative\")\n",
    "            overall_sent = -1\n",
    "        else :\n",
    "            #print(\"Neutral\")\n",
    "            overall_sent = 0\n",
    "    except:\n",
    "        return -2\n",
    "    return overall_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(movies, movie_class, res_file):\n",
    "    llist = []\n",
    "    for m in movies:\n",
    "        m_obj = movie_class[m]\n",
    "        llist.append([m_obj.movie_name, m_obj.tweets, m_obj.pos, m_obj.n, m_obj.neg])\n",
    "    df = pd.DataFrame(llist, columns =['Movie', 'Total Tweets', 'Positive Tweets', 'Neutral Tweets', 'Negative Tweets'])\n",
    "    df.to_csv(res_file, mode='a')\n",
    "    \n",
    "def write_movie_to_file(m_obj, res_file):\n",
    "    llist=[]\n",
    "    llist.append([m_obj.movie_name, m_obj.tweets, m_obj.pos, m_obj.n, m_obj.neg])\n",
    "    df = pd.DataFrame(llist, columns =['Movie', 'Total Tweets', 'Positive Tweets', 'Neutral Tweets', 'Negative Tweets'])\n",
    "    df.to_csv(res_file, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def sentiment_analysis_and_store(tweets_file, res_file):\n",
    "    movies = []\n",
    "    movie_class = {}\n",
    "    with open(tweets_file, 'r', encoding=\"ISO-8859-1\") as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        line_count = 0\n",
    "        for row in csvreader:\n",
    "            if (len(row) < 5):\n",
    "                continue\n",
    "            if line_count < 1:\n",
    "                line_count+=1\n",
    "                continue\n",
    "            movie_name = row[0]\n",
    "            tweet = row[3]\n",
    "#             if line_count < 5:\n",
    "#                 print(tweet)\n",
    "            if tweet == \"\":\n",
    "                continue;\n",
    "            overall_sent = sentiment_scores(tweet)\n",
    "            if (overall_sent == -2):\n",
    "                continue;\n",
    "            if movie_name not in movies:\n",
    "                movie_class[movie_name] = MovieSentiment(movie_name)\n",
    "                if len(movies) >= 1:\n",
    "                    write_movie_to_file(movie_class[movies[-1]], res_file)\n",
    "                movies.append(movie_name)\n",
    "            if overall_sent == 1:\n",
    "                movie_class[movie_name].pos += 1\n",
    "            elif overall_sent == -1:\n",
    "                movie_class[movie_name].neg += 1\n",
    "            elif overall_sent == 0:\n",
    "                movie_class[movie_name].n += 1\n",
    "            movie_class[movie_name].tweets += 1\n",
    "            line_count += 1\n",
    "            if (line_count % 1000 == 0):\n",
    "                print(\"Processed tweet: \", line_count, \"\\n\")\n",
    "    print(f'Processed {line_count - 1} lines.')\n",
    "    file.close()\n",
    "    #write_to_file(movies, movie_class, res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tweet:  1000 \n",
      "\n",
      "Processed tweet:  2000 \n",
      "\n",
      "Processed tweet:  3000 \n",
      "\n",
      "Processed tweet:  4000 \n",
      "\n",
      "Processed tweet:  5000 \n",
      "\n",
      "Processed tweet:  6000 \n",
      "\n",
      "Processed tweet:  7000 \n",
      "\n",
      "Processed tweet:  8000 \n",
      "\n",
      "Processed tweet:  9000 \n",
      "\n",
      "Processed tweet:  10000 \n",
      "\n",
      "Processed tweet:  11000 \n",
      "\n",
      "Processed tweet:  12000 \n",
      "\n",
      "Processed tweet:  13000 \n",
      "\n",
      "Processed tweet:  14000 \n",
      "\n",
      "Processed tweet:  15000 \n",
      "\n",
      "Processed tweet:  16000 \n",
      "\n",
      "Processed tweet:  17000 \n",
      "\n",
      "Processed tweet:  18000 \n",
      "\n",
      "Processed tweet:  19000 \n",
      "\n",
      "Processed tweet:  20000 \n",
      "\n",
      "Processed tweet:  21000 \n",
      "\n",
      "Processed tweet:  22000 \n",
      "\n",
      "Processed tweet:  23000 \n",
      "\n",
      "Processed tweet:  24000 \n",
      "\n",
      "Processed tweet:  25000 \n",
      "\n",
      "Processed tweet:  26000 \n",
      "\n",
      "Processed tweet:  27000 \n",
      "\n",
      "Processed tweet:  28000 \n",
      "\n",
      "Processed tweet:  29000 \n",
      "\n",
      "Processed tweet:  30000 \n",
      "\n",
      "Processed tweet:  31000 \n",
      "\n",
      "Processed tweet:  32000 \n",
      "\n",
      "Processed tweet:  33000 \n",
      "\n",
      "Processed tweet:  34000 \n",
      "\n",
      "Processed tweet:  35000 \n",
      "\n",
      "Processed tweet:  36000 \n",
      "\n",
      "Processed tweet:  37000 \n",
      "\n",
      "Processed tweet:  38000 \n",
      "\n",
      "Processed tweet:  39000 \n",
      "\n",
      "Processed tweet:  40000 \n",
      "\n",
      "Processed tweet:  41000 \n",
      "\n",
      "Processed tweet:  42000 \n",
      "\n",
      "Processed tweet:  43000 \n",
      "\n",
      "Processed tweet:  44000 \n",
      "\n",
      "Processed tweet:  45000 \n",
      "\n",
      "Processed tweet:  46000 \n",
      "\n",
      "Processed tweet:  47000 \n",
      "\n",
      "Processed tweet:  48000 \n",
      "\n",
      "Processed tweet:  49000 \n",
      "\n",
      "Processed tweet:  50000 \n",
      "\n",
      "Processed tweet:  51000 \n",
      "\n",
      "Processed tweet:  52000 \n",
      "\n",
      "Processed tweet:  53000 \n",
      "\n",
      "Processed tweet:  54000 \n",
      "\n",
      "Processed tweet:  55000 \n",
      "\n",
      "Processed tweet:  56000 \n",
      "\n",
      "Processed tweet:  57000 \n",
      "\n",
      "Processed tweet:  58000 \n",
      "\n",
      "Processed tweet:  59000 \n",
      "\n",
      "Processed tweet:  60000 \n",
      "\n",
      "Processed tweet:  61000 \n",
      "\n",
      "Processed tweet:  62000 \n",
      "\n",
      "Processed tweet:  63000 \n",
      "\n",
      "Processed tweet:  64000 \n",
      "\n",
      "Processed tweet:  65000 \n",
      "\n",
      "Processed tweet:  66000 \n",
      "\n",
      "Processed tweet:  67000 \n",
      "\n",
      "Processed tweet:  68000 \n",
      "\n",
      "Processed tweet:  69000 \n",
      "\n",
      "Processed tweet:  70000 \n",
      "\n",
      "Processed tweet:  71000 \n",
      "\n",
      "Processed tweet:  72000 \n",
      "\n",
      "Processed tweet:  73000 \n",
      "\n",
      "Processed tweet:  74000 \n",
      "\n",
      "Processed tweet:  75000 \n",
      "\n",
      "Processed tweet:  76000 \n",
      "\n",
      "Processed tweet:  77000 \n",
      "\n",
      "Processed tweet:  78000 \n",
      "\n",
      "Processed tweet:  79000 \n",
      "\n",
      "Processed tweet:  80000 \n",
      "\n",
      "Processed tweet:  81000 \n",
      "\n",
      "Processed tweet:  82000 \n",
      "\n",
      "Processed tweet:  83000 \n",
      "\n",
      "Processed tweet:  84000 \n",
      "\n",
      "Processed tweet:  85000 \n",
      "\n",
      "Processed tweet:  86000 \n",
      "\n",
      "Processed tweet:  87000 \n",
      "\n",
      "Processed tweet:  88000 \n",
      "\n",
      "Processed tweet:  89000 \n",
      "\n",
      "Processed tweet:  90000 \n",
      "\n",
      "Processed tweet:  91000 \n",
      "\n",
      "Processed tweet:  92000 \n",
      "\n",
      "Processed tweet:  93000 \n",
      "\n",
      "Processed tweet:  94000 \n",
      "\n",
      "Processed tweet:  95000 \n",
      "\n",
      "Processed tweet:  96000 \n",
      "\n",
      "Processed tweet:  97000 \n",
      "\n",
      "Processed tweet:  98000 \n",
      "\n",
      "Processed tweet:  99000 \n",
      "\n",
      "Processed tweet:  100000 \n",
      "\n",
      "Processed tweet:  101000 \n",
      "\n",
      "Processed tweet:  102000 \n",
      "\n",
      "Processed tweet:  103000 \n",
      "\n",
      "Processed tweet:  104000 \n",
      "\n",
      "Processed tweet:  105000 \n",
      "\n",
      "Processed tweet:  106000 \n",
      "\n",
      "Processed tweet:  107000 \n",
      "\n",
      "Processed tweet:  108000 \n",
      "\n",
      "Processed tweet:  109000 \n",
      "\n",
      "Processed tweet:  110000 \n",
      "\n",
      "Processed tweet:  111000 \n",
      "\n",
      "Processed tweet:  112000 \n",
      "\n",
      "Processed tweet:  113000 \n",
      "\n",
      "Processed tweet:  114000 \n",
      "\n",
      "Processed tweet:  115000 \n",
      "\n",
      "Processed tweet:  116000 \n",
      "\n",
      "Processed tweet:  117000 \n",
      "\n",
      "Processed tweet:  118000 \n",
      "\n",
      "Processed tweet:  119000 \n",
      "\n",
      "Processed tweet:  120000 \n",
      "\n",
      "Processed tweet:  121000 \n",
      "\n",
      "Processed 121647 lines.\n",
      "Processing Done For: tweets_2022.csv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'Twitter'\n",
    "files = os.listdir(path)\n",
    "\n",
    "for file in files:\n",
    "        year = file.split(\"_\")[1].split(\".\")[0]\n",
    "        if year in ['2022']:\n",
    "            file_path = os.path.join(path, file)\n",
    "            res_filename = \"Vader_res_\"+year+\".csv\"\n",
    "            sentiment_analysis_and_store(file_path, res_filename)\n",
    "            print(\"Processing Done For:\", file, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
